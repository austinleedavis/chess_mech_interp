{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import chess\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from chess_dataset import ChessDataImporter\n",
    "from mech_interp.fixTL import make_official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"chess_data/\"\n",
    "prefix = \"lichess_\"\n",
    "\n",
    "input_file = f'{DATA_DIR}lichess_uci.csv'\n",
    "output_file = f'{DATA_DIR}lichess_6gb_filtered.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_file):\n",
    "    dataset_path = \"austindavis/chess_mi\"\n",
    "    file_path = \"lichess_uci.zip\"\n",
    "    dataset = load_dataset(dataset_path, data_files=file_path)\n",
    "    df = pd.DataFrame(dataset['train'])\n",
    "    df.to_csv(input_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}{prefix}100mb_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = make_official()\n",
    "tokenizer: PreTrainedTokenizerFast = PreTrainedTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.batch_encode_plus(df['transcript'].tolist(),add_special_tokens=True, return_offsets_mapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_ids'] = encoded['input_ids']\n",
    "df['offsets'] = encoded['offset_mapping']\n",
    "\n",
    "len_df = df['input_ids'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "game_length_in_tokens = 126\n",
    "\n",
    "# # Data setup. All games must have same number of tokens. 50% are >= 134 moves. I will discard all games less than 126, and truncate the rest to 126.\n",
    "filtered_df = df[df['input_ids'].apply(lambda x: len(x) >= game_length_in_tokens)].copy()\n",
    "filtered_df.loc[:, 'input_ids'] = filtered_df['input_ids'].apply(lambda x: x[:game_length_in_tokens])\n",
    "\n",
    "len_df = filtered_df['input_ids'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "# Now we have all games that are encoded to 127 tokens. Need to correct the transcripts accordingly\n",
    "def truncate_transcript(row):\n",
    "    input_ids_length = len(row['input_ids'])\n",
    "    if input_ids_length > 0 and input_ids_length <= len(row['offsets']):\n",
    "        end_char = row['offsets'][input_ids_length - 1][-1]\n",
    "        return row['transcript'][:end_char]\n",
    "    else:\n",
    "        return row['transcript']\n",
    "\n",
    "filtered_df['truncated_transcript'] = filtered_df.apply(truncate_transcript, axis = 1)\n",
    "\n",
    "len_df = filtered_df['transcript'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n",
    "\n",
    "# Finally, I would like to rule out games with promotions because \n",
    "# 1. those games token positions are slightly offset, and\n",
    "# 2. promotion tokens leak information to the model\n",
    "has_promote = filtered_df['truncated_transcript'].apply(\n",
    "    lambda x: any(len(word) == 5 for word in x.split())\n",
    ")\n",
    "filtered_df = filtered_df[~has_promote]\n",
    "len_df = filtered_df['truncated_transcript'].apply(lambda x: len(x))\n",
    "print(len_df.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there's 144,220 games with exactly 312 characters in their truncated transcript. None of these games include pawn promotion, and all of them encode to exactly 126 token_ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Function to create binned columns and bin index columns\n",
    "def create_binned_columns(df, column_name):\n",
    "    binned_column_name = f'{column_name}Binned'\n",
    "    bin_index_column_name = f'{column_name}BinIndex'\n",
    "    \n",
    "    # Create quantile-based bins\n",
    "    num_bins = 6\n",
    "    # Create quantile-based bins with range labels, dropping duplicates if necessary\n",
    "    df[binned_column_name], bins = pd.qcut(df[column_name], q=num_bins, retbins=True, duplicates='drop')\n",
    "\n",
    "    # Convert bin labels to strings and assign to the column\n",
    "    df[binned_column_name] = df[binned_column_name].apply(lambda x: f'({x.left}, {x.right}]')\n",
    "\n",
    "    # Create bin index column\n",
    "    df[bin_index_column_name] = pd.qcut(df[column_name], q=num_bins, labels=False, duplicates='drop')\n",
    "\n",
    "# Apply the function to both WhiteElo and BlackElo\n",
    "create_binned_columns(filtered_df, 'WhiteElo')\n",
    "create_binned_columns(filtered_df, 'BlackElo')\n",
    "\n",
    "filtered_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
    "\n",
    "# Histogram for WhiteElo\n",
    "axes[0].hist(filtered_df['WhiteElo'], bins=30, color='blue', alpha=0.7)\n",
    "axes[0].set_title('WhiteElo Distribution')\n",
    "axes[0].set_xlabel('WhiteElo')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Bar chart for WhiteEloBinned\n",
    "bin_counts = filtered_df['WhiteEloBinned'].value_counts()\n",
    "axes[1].bar(bin_counts.index.astype(str), bin_counts.values, color='green', alpha=0.7)\n",
    "axes[1].set_title('WhiteElo Binned Distribution')\n",
    "axes[1].set_xlabel('WhiteElo Bins')\n",
    "axes[1].set_ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df.rename(columns={'transcript': 'complete_transcript',\n",
    "                                          'truncated_transcript':'transcript'})\n",
    "print(filtered_df['WhiteEloBinned'].value_counts())\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv(output_file,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add FEN board State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129798\n",
      "144220\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{DATA_DIR}{prefix}train.csv')\n",
    "print(len(df))\n",
    "df = pd.concat([df,pd.read_csv(f'{DATA_DIR}{prefix}test.csv')])\n",
    "print(len(df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144220/144220 [23:16<00:00, 103.24it/s]\n"
     ]
    }
   ],
   "source": [
    "from mech_interp.utils import uci_to_board\n",
    "skinny_df = df[['transcript', 'input_ids']]\n",
    "fen_stack = []\n",
    "for transcript in tqdm(skinny_df['transcript']):\n",
    "    board_stack = uci_to_board(transcript.strip(),force=False,fail_silent=True, verbose=False, as_board_stack=True)\n",
    "    fen_stack.append([board.fen() for board in board_stack])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fen_stack'] = fen_stack\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144220\n",
      "129798\n",
      "14422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.read_csv(output_file)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Split df into a train and test split\n",
    "train = df.sample(frac=0.9, random_state=200)\n",
    "test = df.drop(train.index)\n",
    "\n",
    "print(len(train))\n",
    "print(len(test))\n",
    "\n",
    "# # Save the train and test splits to csv\n",
    "# train.to_csv(f'{DATA_DIR}{prefix}train.csv', index=False)\n",
    "# test.to_csv(f'{DATA_DIR}{prefix}test.csv', index=False)\n",
    "\n",
    "train.to_pickle(f'{DATA_DIR}{prefix}train.pkl')\n",
    "test.to_pickle(f'{DATA_DIR}{prefix}test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(df, output_file.replace(\".csv\",\".pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['WhiteElo', 'BlackElo', 'Result', 'complete_transcript', 'input_ids',\n",
       "       'offsets', 'transcript', 'WhiteEloBinned', 'WhiteEloBinIndex',\n",
       "       'BlackEloBinned', 'BlackEloBinIndex', 'fen_stack'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['input_ids'] = df['input_ids'].apply(lambda x: [int(i) for i in x.strip(\"[]\").split(', ')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df['offsets'] = df['offsets'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['fen_stack'] = df['fen_stack'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WhiteElo: <class 'numpy.int64'>\n",
      "BlackElo: <class 'numpy.int64'>\n",
      "Result: <class 'str'>\n",
      "complete_transcript: <class 'str'>\n",
      "input_ids: <class 'list'>\n",
      "offsets: <class 'list'>\n",
      "transcript: <class 'str'>\n",
      "WhiteEloBinned: <class 'str'>\n",
      "WhiteEloBinIndex: <class 'numpy.int64'>\n",
      "BlackEloBinned: <class 'str'>\n",
      "BlackEloBinIndex: <class 'numpy.int64'>\n",
      "fen_stack: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "for col in test2.columns:\n",
    "    print(f\"{col}: {type(df[col][0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
